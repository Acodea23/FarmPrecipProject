{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647baecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915dcfb4",
   "metadata": {},
   "source": [
    "# Precip Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7259b3dc",
   "metadata": {},
   "source": [
    "### Scraping data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c254c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.ncei.noaa.gov/pub/data/cirs/drd/drd964x.pdsi.txt\"\n",
    "\n",
    "\n",
    "#picking names for files to save. both a txt and a csv\n",
    "txt_name = \"rain.txt\"\n",
    "csv_name = \"rain_dirty.csv\"\n",
    "\n",
    "# colspecs and cols is formatting specific to this file\n",
    "#the first col of the txt contains several pieces of information in a string\n",
    "colspecs = [\n",
    "    (0, 2), (2, 4), (4, 6), (6, 10),\n",
    "    (10, 17), (17, 24), (24, 31), (31, 38),\n",
    "    (38, 45), (45, 52), (52, 59), (59, 66),\n",
    "    (66, 73), (73, 80), (80, 87), (87, 94)\n",
    "]\n",
    "\n",
    "cols = [\n",
    "    \"state\", \"division\", \"element\", \"year\",\n",
    "    \"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\n",
    "    \"sep\",\"oct\",\"nov\",\"dec\"\n",
    "]\n",
    "\n",
    "\n",
    "# takes input that is requests.get()\n",
    "# saves txt file from it. no return value\n",
    "def write_txt(txt_name, r):\n",
    "    with open(txt_name, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "\n",
    "# no input, reads txt file, converts strings to numeric values, writes csv with numeric values\n",
    "def txt_to_csv(txt_name, csv_name, colspecs, cols):\n",
    "    df = pd.read_fwf(txt_name, colspecs=colspecs, names=cols)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.to_csv(csv_name, index=False)\n",
    "\n",
    "\n",
    "# takes a url as an input\n",
    "#checks status code = 200 to read it\n",
    "#takes data from website and writes to a txt file and a csv\n",
    "# MUST BE A TXT FILE URL!!!\n",
    "def read_url_txt(url, txt_name, csv_name, colspecs, cols):\n",
    "    # if url[-4] != \".txt\":\n",
    "    #     print(\"url must be a txt\")\n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"url status code is {r.status_code} not 200. Please check your url\")\n",
    "        return\n",
    "    write_txt(txt_name, r)\n",
    "    txt_to_csv(txt_name, colspecs, cols, csv_name)\n",
    "\n",
    "read_url_txt(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1def6705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |   0101051895 |   0.11 |   -0.81 |   -0.56 |   -0.72 |   -0.85 |   -0.99 |   -0.90 |   -1.13 |   -1.69 |   -1.72 |   -2.04 |   -2.12 |\n",
      "|---:|-------------:|-------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|\n",
      "|  0 |  1.01052e+08 |  -2.47 |   -2.06 |   -1.75 |   -1.84 |   -2.16 |   -1.74 |   -1.53 |   -2.1  |   -2.23 |   -2.41 |   -1.61 |   -2.53 |\n",
      "|  1 |  1.01052e+08 |  -2.45 |    0.05 |    1.48 |    1.98 |   -0.51 |   -1.02 |   -1.39 |   -1.5  |   -2.49 |   -3.02 |   -3.23 |   -2.63 |\n",
      "|  2 |  1.01052e+08 |  -1.73 |   -2.43 |   -2.75 |   -2.09 |   -2.6  |   -2.72 |   -2.65 |   -2.47 |   -2.07 |   -1.67 |   -1.51 |   -1.97 |\n",
      "|  3 |  1.01052e+08 |   0.15 |    0.46 |    0.96 |   -0.24 |   -0.89 |   -1.51 |   -1.57 |   -2.1  |   -2.66 |   -2.85 |   -3.31 |   -2.88 |\n",
      "|  4 |  1.01052e+08 |  -3.13 |   -2.41 |   -2.61 |   -2.15 |   -2.38 |    2.31 |   -0.12 |   -0.67 |   -0.61 |    0.34 |    0.75 |   -0.57 |\n"
     ]
    }
   ],
   "source": [
    "#print(pd.read_fwf(txt_name).head().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49a75a",
   "metadata": {},
   "source": [
    "### Cleaning data\n",
    "Make a new csv that contains normalized precip data by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c906add0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"rain_dirty.csv\").drop([\"division\", \"element\"], axis = 1)\n",
    "\n",
    "months = [\"jan\",\"feb\",\"mar\",'apr',\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
    "groups = [\"state\",\"year\"]\n",
    "\n",
    "csv_name_clean = \"rain_clean.csv\"\n",
    "\n",
    "# takes are precip data, combines the months, and divisions to get average rain per state per year. After I normalize it\n",
    "def normalized_data(df, months, groups):\n",
    "    #column with yearly average, combines months\n",
    "    df[\"yearly_avg\"] = df[months].mean(axis=1)\n",
    "    \n",
    "    # combines averages across divisions of the state. now average per state\n",
    "    #then normalizes values across states and years (x-mean)/std\n",
    "    \n",
    "    state_precip = df.groupby(groups)[\"yearly_avg\"].mean()\n",
    "    #no need to normlaize, PDSI is already scaled by state\n",
    "    #state_precip[\"stand_precip\"] = (state_precip[\"avg_precip\"] - state_precip[\"avg_precip\"].mean()) / state_precip[\"avg_precip\"].std()\n",
    "\n",
    "    #writes df to csv\n",
    "    state_precip.to_csv(csv_name_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f840f2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |   state |   year |   yearly_avg |\n",
      "|---:|--------:|-------:|-------------:|\n",
      "|  0 |       1 |   1895 |    -0.821354 |\n"
     ]
    }
   ],
   "source": [
    "normalized_data(df, months, groups)\n",
    "#print(pd.read_csv(\"rain_clean.csv\").head(1).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b54196",
   "metadata": {},
   "source": [
    "# Farm data - NOT USED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc66244",
   "metadata": {},
   "outputs": [],
   "source": [
    "farm_data = pd.read_excel(\"VA_State_US.xlsx\")\n",
    "#farm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ca559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = \"VA_State_US.xlsx\"\n",
    "\n",
    "# Read all sheets\n",
    "all_sheets = pd.read_excel(excel_path, sheet_name=None)\n",
    "\n",
    "all_state = pd.DataFrame()\n",
    "# `all_sheets` is a dictionary: {sheet_name: DataFrame}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c1ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_sheets.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9772a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in all_sheets.items():\n",
    "    # print(f\"Sheet: {name}\")\n",
    "    # print(df.head())\n",
    "    print(name)\n",
    "    if name != \"Document Map\":\n",
    "        if name != \"United States\":\n",
    "            df[\"state\"] = name\n",
    "            # removes rows where first column is NaN\n",
    "            #df.drop([0,2,3,16,25,34,36,55,57,62,66,72,74,75,76])\n",
    "            #df = df[df.iloc[:, 1].notna()]\n",
    "            df.to_csv(f\"state_farm_data/{name}.csv\")\n",
    "\n",
    "            all_state = pd.concat([all_state,df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8eee075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_state.head()\n",
    "all_state.to_csv(\"all_state.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c03a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_df = pd.concat(all_sheets.values(), ignore_index=True)\n",
    "print(combined_df.head())\n",
    "combined_df.to_csv(\"test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff09dc",
   "metadata": {},
   "source": [
    "## Joining farm and precip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76c70f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_farm_clean = pd.read_csv(\"FarmIncome_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6903f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#state_precip = pd.read_csv(\"rain_clean.csv\")\n",
    "\n",
    "csvs = [\"FarmIncome_full.csv\", \"rain_clean.csv\"]\n",
    "group_on = [\"state\", \"year\"]\n",
    "new_csv_name = \"combined_farm_precip.csv\"\n",
    "\n",
    "def merge_csvs(csvs, group_on, new_csv_name):\n",
    "    df1 = pd.read_csv(csvs[0])\n",
    "    df2 = pd.read_csv(csvs[1])\n",
    "    merged_df = pd.merge(df1, df2, on=group_on)\n",
    "    merged_df.to_csv(new_csv_name)\n",
    "    #return merged_df\n",
    "\n",
    "merge_csvs(csvs, group_on, new_csv_name)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FarmPrecipProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
