[
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "getting started",
    "section": "",
    "text": "Here is how you use my package:\n\nfrom farm_precip_project import write_txt\nprint(\"Hello World\")\n\nHello World"
  },
  {
    "objectID": "usda_scrape.html",
    "href": "usda_scrape.html",
    "title": "Precip Data",
    "section": "",
    "text": "import pandas as pd\nfrom bs4 import BeautifulSoup\nimport numpy as np\nimport requests\nimport re"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "usda_scrape.html#joining-farm-and-precip-data",
    "href": "usda_scrape.html#joining-farm-and-precip-data",
    "title": "Precip Data",
    "section": "Joining farm and precip data",
    "text": "Joining farm and precip data\n\nfull_farm_clean = pd.read_csv(\"FarmIncome_full.csv\")\n\n\n#state_precip = pd.read_csv(\"rain_clean.csv\")\n\ncsvs = [\"FarmIncome_full.csv\", \"rain_clean.csv\"]\ngroup_on = [\"state\", \"year\"]\nnew_csv_name = \"combined_farm_precip.csv\"\n\ndef merge_csvs(csvs, group_on, new_csv_name):\n    df1 = pd.read_csv(csvs[0])\n    df2 = pd.read_csv(csvs[1])\n    merged_df = pd.merge(df1, df2, on=group_on)\n    merged_df.to_csv(new_csv_name)\n    #return merged_df\n\nmerge_csvs(csvs, group_on, new_csv_name)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Farm Precipition with Income Package",
    "section": "",
    "text": "Here is a website about our python package\nDocumentation here.\nGet started here."
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Data",
    "section": "",
    "text": "Data\n\nSources\nPrecipitation - NOAA (National Oceanic and Atmospheric Administration). This data uses the PDSI metric. Which is an indicator of how much water is available compared to the average amount of water normally available in that region. It factors in precipitation, temperature, and soil moisture. This data tells us the monthly PDSI of each region of each state for each year. It is reasonable to average across regions, because for each state the regions are roughy equal in size.\nFarm Income - USDA (United States Department of Agriculture). This website contains an excel spreadsheet that can be downloaded. This excel file contians a sheet for each state. Each state’s sheet contains a column per year where each row contains dollar values for different parts of a State’s agricultural profits. It includes crop, livestock, and expense data among other things.\n\n\nFile Descriptions\nThe Precipitation dataset is a .txt file. It’s first column contains a code for the state, division, element (in this case PDSI), and year. It then contains a column for each month, which contains the PDSI value for that month.\nAn example row from the txt file:\n0101051895 | 0.11 | -0.81 | -0.56 | -0.72 | -0.85 | -0.99 | -0.90 | -1.13 | -1.69 | -1.72 | -2.04 | -2.12 |\nState code = 01 (which corresponds to a state) Division number = 01 (first climate division in Florida) Element code = 05 (PDSI) Year = 1895 January PDSI value = 0.11 February PDSI value = -0.81 March PDSI value = -0.56 … and so forth through December.\n\n\nCombined Data File\n\n\n\nFunctions\n\nPrecipitation/PDSI Data Preparation\nThese functions will use a url file to produce write multiple files. The raw output from the url to a txt and csv file. It then produces a table that provides the average PDSI per state per year and writes it to a csv.\n\nread_url_txt(url, txt_name, csv_name, colspecs, cols)\n\nOverview:\nInput is a url containing a .txt file. Uses requests package to have requests.get output. Uses write_txt() and txt_to_csv() (see below) to save .txt and .csv files from dataset.\nArgs:\nurl:str - a url pointing to a remote .txt file. (“https://www.ncei.noaa.gov/pub/data/cirs/drd/drd964x.pdsi.txt”)\ntxt_name:str - the name the .txt file will be saved to. (“name.txt”)\ncsv_name:str - the name the .csv file will be saved to. (“name.csv”)\ncol_specs:list - contains tuples that indicate how to break string into parts. Must be same length as cols.([(0, 2), (2, 4), (4, 6), (6, 10),(10, 17)])\ncols:list - contains names of new columns that are created with col_specs. Must be same length as col_specs. (cols = [“col1”, “col2”, “col3”, “col4”,“col5”])\nImpact: Returns None. This function will write files, but will not return anything.\nExample:\nimport requests\nimport pandas\nfrom farm_precip_project import read_url_txt, write_txt, txt_to_csv\n\n# Step 1: Fetch remote data\nurl = \"https://example.org/pdsi_data_2020.txt\"\nr = requests.get(url)\n\nif r.status_code == 200:\n    # Step 2: Save raw text\n    write_txt(\"pdsi_2020.txt\", r)\n\n    # Step 3: Parse fixed-width into CSV\n    # Suppose the first column encodes multiple indicators; we split using colspecs:\n    colspecs = [(0, 4), (4, 8), (8, 12)]  # start-inclusive, end-exclusive\n    cols     = [\"state_code\", \"district_code\", \"indicator\"]\n    txt_to_csv(\"pdsi_2020.txt\", colspecs, cols, \"pdsi_2020.csv\")\n\nwrite_txt(txt_name = “string.txt”, r = requests.get output) Takes input an input that is requests.get(). Saves txt file from website. Does not return a value.\ntxt_to_csv(txt_name = “string.txt”, colspecs = [contains tuples breaking up first column’s indicators], cols = [contains column names as strings], csv_name = “csv_string.csv”) Reads txt file. Creates new columns from first column, breaks up indicator into columns. Converts strings cells containing PDSI to numeric values, Writes new table to csv.\nnormalized_data(df = pandas dataframe, months = [strings of names of months used in cols list above], groups = [strings of cols to group by]) Averages the mean PDSI across months for each row, saves it as a column. This is the yearly average PDSI for each district of each state. Then creates a new table averaging the PDSI yearly PDSI for every state across all its districts. Writes the table to a csv.\n\n\n\nPrecipitation/PDSI Data Tranforming\n\n\n\nFiles"
  },
  {
    "objectID": "documentation.html#read_url_txt",
    "href": "documentation.html#read_url_txt",
    "title": "Overview",
    "section": "",
    "text": "Signature\nread_url_txt(url)\nOverview\nInput is a URL containing a .txt file. Internally uses requests.get to fetch the content, then calls write_txt() and txt_to_csv() to save the .txt and .csv files from the dataset.\nParameters - url : str\nA URL pointing to a remote text file (e.g., \"https://example.com/data.txt\").\nReturns\n- None\nThis function orchestrates fetching and saving; its side-effects are writing files to disk.\nSide Effects - Saves the fetched text as a local .txt file via write_txt(). - Converts the .txt to a parsed .csv via txt_to_csv().\nNotes - The function expects the remote resource to return HTTP status code 200. If status_code != 200, a message is printed, and the operation is aborted. - Because read_url_txt delegates to write_txt() and txt_to_csv(), it should pass the appropriate arguments such as a destination txt_name, and the parsing instructions (colspecs, cols, csv_name) to fully complete the pipeline. - Typical pitfalls: - Using a URL that does not point to a .txt file (or serves a different content type). - Forgetting to specify consistent colspecs/cols downstream, which are required by txt_to_csv().\nExample ```python import requests from your_package import read_url_txt, write_txt, txt_to_csv"
  }
]