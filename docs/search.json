[
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Example Usage",
    "section": "",
    "text": "Here is an example of how the functions in our package can work together.\n\nimport pandas as pd\nimport requests\n\n\nPDSI Data\nWe start by using our functions to scrape the data, and save it to a csv.\n\n# remember txt_to_csv is called in read_url_txt\nfrom farm_precip_project import read_url_txt\n\nurl = \"https://www.ncei.noaa.gov/pub/data/cirs/drd/drd964x.pdsi.txt\"\n\n\n#picking names for files to save. both a txt and a csv\ntxt_name = \"rain.txt\"\ncsv_name = \"rain_dirty.csv\"\n\n# colspecs and cols is formatting specific to this file\n#the first col of the txt contains several pieces of information in a string\ncolspecs = [\n    (0, 2), (2, 4), (4, 6), (6, 10),\n    (10, 17), (17, 24), (24, 31), (31, 38),\n    (38, 45), (45, 52), (52, 59), (59, 66),\n    (66, 73), (73, 80), (80, 87), (87, 94)\n]\n\ncols = [\n    \"state\", \"division\", \"element\", \"year\",\n    \"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\n    \"sep\",\"oct\",\"nov\",\"dec\"\n]\n\nread_url_txt(url, txt_name, csv_name, colspecs, cols)\n\n# The head of the data saved as a .csv\nprint(pd.read_csv(csv_name).head(1).to_markdown())\n\n|    |   state |   division |   element |   year |   jan |   feb |   mar |   apr |   may |   jun |   jul |   aug |   sep |   oct |   nov |   dec |\n|---:|--------:|-----------:|----------:|-------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|\n|  0 |       1 |          1 |         5 |   1895 |  0.11 | -0.81 | -0.56 | -0.72 | -0.85 | -0.99 |  -0.9 | -1.13 | -1.69 | -1.72 | -2.04 | -2.12 |\n\n\nWe then calculate yearly averages for each state and save it in a new csv file. In this case we will read the csv created using read_url_txt and perform the calculations on it.\n\nfrom farm_precip_project import normalized_data\n\nmonths = [\"jan\",\"feb\",\"mar\",'apr',\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\ngroups = [\"state\",\"year\"]\nnew_col_name = \"yearly_avg\"\ncsv_name_clean = \"rain_clean.csv\"\ndf_to_read = \"rain_dirty.csv\"\n\nnormalized_data(df_to_read, new_col_name, csv_name_clean, months, groups)\n\nprint(pd.read_csv(csv_name).head(1).to_markdown())\n\n|    |   state |   division |   element |   year |   jan |   feb |   mar |   apr |   may |   jun |   jul |   aug |   sep |   oct |   nov |   dec |\n|---:|--------:|-----------:|----------:|-------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|\n|  0 |       1 |          1 |         5 |   1895 |  0.11 | -0.81 | -0.56 | -0.72 | -0.85 | -0.99 |  -0.9 | -1.13 | -1.69 | -1.72 | -2.04 | -2.12 |\n\n\n\n\nFarm Income Data (Katie Here)\n\n\nMerging the Data\nWe then use merge_csvs to combine our farm and PDSI datasets into one csv.\n\nfrom farm_precip_project import merge_csvs\nimport pandas as pd\n\ncsvs = [\"FarmIncome_full.csv\", \"rain_clean.csv\"]\ngroup_on = [\"state\", \"year\"]\nnew_csv_name = \"combined_farm_precip.csv\"\n\nmerge_csvs(new_csv_name, csvs, group_on)\n\nprint(pd.read_csv(new_csv_name).head(1).to_markdown())\n\n|    |   state |   year |   Value of crop production |   Crop cash receipts |   Cotton |   Feed crops |   Food grains |   Fruits and nuts |   Oil crops |   Vegetables and melons |   All other crops |   Home consumption |   Inventory adjustment |   yearly_avg |\n|---:|--------:|-------:|---------------------------:|---------------------:|---------:|-------------:|--------------:|------------------:|------------:|------------------------:|------------------:|-------------------:|-----------------------:|-------------:|\n|  0 |       1 |   1949 |                     250813 |               230275 |   150706 |        13405 |           231 |              4937 |       30196 |                   14421 |             16238 |              39680 |                 -19142 |      1.18625 |\n\n\n\n\nEDA (Katie Here)\n\n\nAnalysis\nWe now want to perform some analysis to better understand the relationship between income and PDSI values.\n\nData cleaning from EDA findings\nWhen performing EDA we discovered that the year 2014 had errors in their data collection that caused all states to have unrealistic values. So we will use our remove_outliers function to form a dataframe without the incorrect data.\n\nfrom farm_precip_project import remove_outliers\nimport pandas as pd\n\ndf = pd.read_csv('combined_farm_precip.csv')\ncol_name = \"yearly_avg\"\nthreshold = -50\n\ndf = remove_outliers(df, col_name, threshold, lower = True)\n\n\n\nAnalyzing data\nWe now want to explore the relationship between the PDSI, and the agricultural income. So we will use corr_and_plot to produce a scatterplot of the two variable, and a correlation coefficient.\n\nfrom farm_precip_project import corr_and_plot\nimport matplotlib.pyplot as plt\n\n# the df is pulled from the above cell where we cleaned the incorrect values\ncolx = \"yearly_avg\"\ncoly = \"Value of crop production\"\nplot_file = f\"plot_{colx}_vs_{coly}.png\"\nn_digits = 4\n\ncorr_and_plot(df, colx, coly, plot_file,n_digits)\n\n\n\n\n\n\n\n\nnp.float64(0.0526)\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nIt appears there is not a correlation between farming income and the PDSI. This is due to different states having different average income levels. Thus we need to center the income for each state, as then we can focus on exploring the relationship between the change in income and change in PDSI. We will use center_column to do so.\n\nfrom farm_precip_project import center_column\nimport pandas as pd\n\ncol_name = \"yearly_avg\"\ncol_group = \"state\"\ncol_stand_name = \"income_centered\"\n\ndf = center_column(df, col_name, col_group, col_stand_name)\n\nprint(df.head(5).to_markdown())\n\n|    |   state |   year |   Value of crop production |   Crop cash receipts |   Cotton |   Feed crops |   Food grains |   Fruits and nuts |   Oil crops |   Vegetables and melons |   All other crops |   Home consumption |   Inventory adjustment |   yearly_avg |   income_centered |\n|---:|--------:|-------:|---------------------------:|---------------------:|---------:|-------------:|--------------:|------------------:|------------:|------------------------:|------------------:|-------------------:|-----------------------:|-------------:|------------------:|\n|  0 |       1 |   1949 |                     250813 |               230275 |   150706 |        13405 |           231 |              4937 |       30196 |                   14421 |             16238 |              39680 |                 -19142 |     1.18625  |          1.07534  |\n|  1 |       1 |   1950 |                     267006 |               229713 |   130815 |        13510 |           245 |              4850 |       32003 |                   12494 |             35796 |              39254 |                  -2143 |    -0.492812 |         -0.60372  |\n|  2 |       1 |   1951 |                     292913 |               264151 |   177427 |        14241 |           192 |              6245 |       23205 |                   13414 |             29427 |              35748 |                  -7278 |    -1.1524   |         -1.2633   |\n|  3 |       1 |   1952 |                     298393 |               277208 |   189744 |         9517 |           268 |              5274 |       25783 |                   20143 |             26479 |              49808 |                 -28897 |    -1.60104  |         -1.71195  |\n|  4 |       1 |   1953 |                     331899 |               263726 |   180347 |         9779 |           682 |              7388 |       24700 |                   16534 |             24296 |              46457 |                  21395 |    -0.649375 |         -0.760282 |\n\n\nNow we will once again use our corr_and_plot function to analyze the relationship.\n\nfrom farm_precip_project import corr_and_plot\nimport matplotlib.pyplot as plt\n\ncolx = \"yearly_avg\"\ncoly = \"income_centered\"\nplot_file = f\"plot_{colx}_vs_{coly}.png\"\nn_digits = 4\n\ncorr_and_plot(df, colx, coly, plot_file,n_digits)\n\n\n\n\n\n\n\n\nnp.float64(0.9794)\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nWe now see an apparent linear relationship between the two variables. We now want to see if this is actually a group of lines where each line is a state. This would let us know if certain states are less affected by PDSI than other states. We can use make_scatter_w_cat to create a scatterplot that colors the points by category.\n\nfrom farm_precip_project import make_scatter_w_cat\nimport seaborn as sns\n\ncolx = \"yearly_avg\"\ncoly = \"income_centered\"\ncolcat = \"state\"\nplot_file = f\"plot_{colx}_vs_{coly}_by_{colcat}.png\"\n\nmake_scatter_w_cat(df, colx, coly, colcat, plot_file)\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nThrough this plot we can see that each state appears to have a line of its points with a similar slope to each other state. The difference is the line‚Äôs interecept. This causes us to note that some states perform above their average income with an average PDSI while other perform below their average income with an average PDSI. It is a relationship that would be worth exploring in the future."
  },
  {
    "objectID": "usda_scrape.html",
    "href": "usda_scrape.html",
    "title": "Precip Data",
    "section": "",
    "text": "import pandas as pd\n# from bs4 import BeautifulSoup\n# import numpy as np\nimport requests\n# import re"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "usda_scrape.html#joining-farm-and-precip-data",
    "href": "usda_scrape.html#joining-farm-and-precip-data",
    "title": "Precip Data",
    "section": "Joining farm and precip data",
    "text": "Joining farm and precip data\n\nfull_farm_clean = pd.read_csv(\"FarmIncome_full.csv\")\n\n\n#state_precip = pd.read_csv(\"rain_clean.csv\")\n\ncsvs = [\"FarmIncome_full.csv\", \"rain_clean.csv\"]\ngroup_on = [\"state\", \"year\"]\nnew_csv_name = \"combined_farm_precip.csv\"\n\ndef merge_csvs(csvs, group_on, new_csv_name):\n    df1 = pd.read_csv(csvs[0])\n    df2 = pd.read_csv(csvs[1])\n    merged_df = pd.merge(df1, df2, on=group_on)\n    merged_df.to_csv(new_csv_name)\n    #return merged_df\n\nmerge_csvs(csvs, group_on, new_csv_name)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Farm Precipitation & Income Analysis",
    "section": "",
    "text": "Explore the relationship between precipitation patterns and farm income data. This project analyzes how weather conditions impact agricultural productivity and income across different regions and time periods."
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Documentation For Package",
    "section": "",
    "text": "Data\n\nSources\nPrecipitation - NOAA (National Oceanic and Atmospheric Administration). This data uses the PDSI metric. Which is an indicator of how much water is available compared to the average amount of water normally available in that region. It factors in precipitation, temperature, and soil moisture. This data tells us the monthly PDSI of each region of each state for each year. It is reasonable to average across regions, because for each state the regions are roughy equal in size.\nFarm Income - USDA (United States Department of Agriculture). This website contains an excel spreadsheet that can be downloaded. This excel file contians a sheet for each state. Each state‚Äôs sheet contains a column per year where each row contains dollar values for different parts of a State‚Äôs agricultural profits. It includes crop, livestock, and expense data among other things.\n\n\nFile Descriptions\nThe Precipitation dataset is a .txt file. It‚Äôs first column contains a code for the state, division, element (in this case PDSI), and year. It then contains a column for each month, which contains the PDSI value for that month.\nAn example row from the txt file: \n0101051895 | 0.11 | -0.81 | -0.56 | -0.72 | -0.85 | -0.99 | -0.90 | -1.13 | -1.69 | -1.72 | -2.04 | -2.12 |\nState code = 01 (which corresponds to a state) Division number = 01 (first climate division in Florida) Element code = 05 (PDSI) Year = 1895 January PDSI value = 0.11 February PDSI value = -0.81 March PDSI value = -0.56 ‚Ä¶ and so forth through December.\nThe Farm Income dataset is an .xlsx file made up of several sheets, one for each state as well as informational guides. It contains rows of data separated by year in columns, containing income data across several different industry types. We chose to focus on food-type crops, found in the first block-like row section of each state sheet.\nAn example row from the text file:\n\n# import pandas as pd\n\n\n# # read the \"Alabama\" sheet and show the first few rows\n# df = pd.read_excel(\"VA_State_US (1).xlsx\", sheet_name=\"Alabama\")\n# print(df.head(1).to_markdown())\n\n\n\nCombined Data File\nThe combined dataset contains information from both datasets joined on the state and the year. It provides PDSI averages for the state along with the state‚Äôs to total income, and income by crop category. An example row of the data is included below.\n\nimport pandas as pd\nprint(pd.read_csv(\"combined_farm_precip.csv\").head(1).to_markdown())\n\n|    |   state |   year |   Value of crop production |   Crop cash receipts |   Cotton |   Feed crops |   Food grains |   Fruits and nuts |   Oil crops |   Vegetables and melons |   All other crops |   Home consumption |   Inventory adjustment |   yearly_avg |\n|---:|--------:|-------:|---------------------------:|---------------------:|---------:|-------------:|--------------:|------------------:|------------:|------------------------:|------------------:|-------------------:|-----------------------:|-------------:|\n|  0 |       1 |   1949 |                     250813 |               230275 |   150706 |        13405 |           231 |              4937 |       30196 |                   14421 |             16238 |              39680 |                 -19142 |      1.18625 |\n\n\n\n\n\nFunctions\n\nPrecipitation/PDSI Data Preparation\nThese functions will use a url file to produce write multiple files. The raw output from the url to a txt and csv file. It then produces a table that provides the average PDSI per state per year and writes it to a csv.\n\nread_url_txt(url, txt_name, csv_name, colspecs, cols)\n\nOverview:\nInput is a url containing a .txt file. Uses requests package to have requests.get output. Saves a .csv file and uses txt_to_csv() (see below) to save .txt file of dataset. Calls txt_to_csv as part of process.\nArgs:\nurl:str - a url pointing to a remote .txt file. (‚Äúhttps://www.ncei.noaa.gov/pub/data/cirs/drd/drd964x.pdsi.txt‚Äù) txt_name:str - the name the .txt file will be saved to. (‚Äúname.txt‚Äù) csv_name:str - the name the .csv file will be saved to. (‚Äúname.csv‚Äù) col_specs:list - contains tuples that indicate how to break string into parts. Must be same length as cols.([(0, 2), (2, 4), (4, 6), (6, 10),(10, 17)]) cols:list - contains names of new columns that are created with col_specs. Must be same length as col_specs. (cols = [‚Äúcol1‚Äù, ‚Äúcol2‚Äù, ‚Äúcol3‚Äù, ‚Äúcol4‚Äù,‚Äúcol5‚Äù])\nImpact: Returns None. This function will write files, but will not return anything.\n\ntxt_to_csv(txt_name, csv_name, colspecs, cols)\n\nOverview:\nReads txt file. Parses government coding system from first column into various columns. Converts strings cells containing PDSI information to numeric values, Writes new broken up table to csv.\nArgs: txt_name:str - the name the .txt file will be saved to. (‚Äúname.txt‚Äù) csv_name:str - the name the .csv file will be saved to. (‚Äúname.csv‚Äù) col_specs:list - contains tuples that indicate how to break string into parts. Must be same length as cols.([(0, 2), (2, 4), (4, 6), (6, 10),(10, 17)]) cols:list - contains names of new columns that are created with col_specs. Must be same length as col_specs. (cols = [‚Äúcol1‚Äù, ‚Äúcol2‚Äù, ‚Äúcol3‚Äù, ‚Äúcol4‚Äù,‚Äúcol5‚Äù])\nImpact: Returns None. This function will write files, but will not return anything.\nExample:\nimport requests\nimport pandas as pd\nfrom farm_precip_project import read_url_txt, txt_to_csv\n\nurl = \"https://www.ncei.noaa.gov/pub/data/cirs/drd/drd964x.pdsi.txt\"\n\ntxt_name = \"rain.txt\"\ncsv_name = \"rain_dirty.csv\"\n\ncolspecs = [\n    (0, 2), (2, 4), (4, 6), (6, 10),\n    (10, 17), (17, 24), (24, 31), (31, 38),\n    (38, 45), (45, 52), (52, 59), (59, 66),\n    (66, 73), (73, 80), (80, 87), (87, 94)\n]\n\ncols = [\n    \"state\", \"division\", \"element\", \"year\",\n    \"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\n    \"sep\",\"oct\",\"nov\",\"dec\"\n]\n\nread_url_txt(url, txt_name, csv_name, colspecs, cols)\n\nprint(pd.read_csv(csv_name).head(1).to_markdown())\n\nnormalized_data(df, months, groups, csv_name_clean, new_col_name)\n\nOverview: Averages the PDSI across months for each row, and saves it as a new column. This is the yearly average PDSI for each district of each state. Creates a new table averaging the yearly PDSI for every state across all its districts. Writes the table to a csv.\nArgs: df_to_read:str - contains name of csv to read. new_col_name:str - name of new column that is added when calculating average for each row. csv_name_clean:str - name of csv you want to save your calculated table to. (‚Äúaveraged_vals.csv‚Äù) months:list - contains strings of values that will be used to calculate the averages for each row. ([‚Äúcol1‚Äù,‚Äúcol2‚Äù,‚Äúcol3‚Äù]) groups:list - contains strings of values to group by when calculating averages across rows. ([‚Äúgroup1‚Äù, ‚Äúgroup2‚Äù])\nImpact: Returns None. This function will write files, but will not return anything.\nExample:\n\nimport pandas as pd\nfrom farm_precip_project import normalized_data\n\nmonths = [\"jan\",\"feb\",\"mar\",'apr',\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\ngroups = [\"state\",\"year\"]\nnew_col_name = \"yearly_avg\"\ncsv_name_clean = \"rain_clean.csv\"\ndf_to_read = \"rain_dirty.csv\"\n\nnormalized_data(df_to_read, new_col_name, csv_name_clean, months, groups)\nprint(pd.read_csv(csv_name_clean).head(1).to_markdown())\n\n|    |   state |   year |   yearly_avg |\n|---:|--------:|-------:|-------------:|\n|  0 |       1 |   1895 |    -0.821354 |\n\n\n\n\nFarm Data Preparation (Katie Here)\n\n\nCombining data\nWe created a function to merge two datasets on specific values and to save the merged data in a csv.\n\nmerge_csvs(new_csv_name, csvs, group_on)\n\nOverview: Reads two .csv files. Merges the files on a set of columns. Writes the merged table to a .csv.\nArgs: new_csv_name:str Name to use for merged .csv file (‚Äúmerged.csv‚Äù) csvs:list Contains two strings where each string is a .csv to be read and merged ([‚Äútble1.csv‚Äù, ‚Äútble2.csv‚Äù]) group_on:list Contains strings where each string is a column to be grouped on ([‚Äúcol1‚Äù, ‚Äúcol2‚Äù, ‚Äúcol3‚Äù])\nImpact: Returns None. Writes a new .csv file\nExample:\nStarting csvs:\n\ncsvs = [\"FarmIncome_full.csv\", \"rain_clean.csv\"]\nprint(pd.read_csv(csvs[0]).head(1).to_markdown())\nprint(pd.read_csv(csvs[1]).head(1).to_markdown())\n\n|    |   state |   year |   Value of crop production |   Crop cash receipts |   Cotton |   Feed crops |   Food grains |   Fruits and nuts |   Oil crops |   Vegetables and melons |   All other crops |   Home consumption |   Inventory adjustment |\n|---:|--------:|-------:|---------------------------:|---------------------:|---------:|-------------:|--------------:|------------------:|------------:|------------------------:|------------------:|-------------------:|-----------------------:|\n|  0 |       1 |   1949 |                     250813 |               230275 |   150706 |        13405 |           231 |              4937 |       30196 |                   14421 |             16238 |              39680 |                 -19142 |\n|    |   state |   year |   yearly_avg |\n|---:|--------:|-------:|-------------:|\n|  0 |       1 |   1895 |    -0.821354 |\n\n\nMerging them:\n\nimport pandas as pd\nfrom farm_precip_project import merge_csvs\n\ncsvs = [\"FarmIncome_full.csv\", \"rain_clean.csv\"]\ngroup_on = [\"state\", \"year\"]\nnew_csv_name = \"combined_farm_precip.csv\"\n\nmerge_csvs(new_csv_name, csvs, group_on)\n\nprint(pd.read_csv(new_csv_name).head(1).to_markdown())\n\n|    |   state |   year |   Value of crop production |   Crop cash receipts |   Cotton |   Feed crops |   Food grains |   Fruits and nuts |   Oil crops |   Vegetables and melons |   All other crops |   Home consumption |   Inventory adjustment |   yearly_avg |\n|---:|--------:|-------:|---------------------------:|---------------------:|---------:|-------------:|--------------:|------------------:|------------:|------------------------:|------------------:|-------------------:|-----------------------:|-------------:|\n|  0 |       1 |   1949 |                     250813 |               230275 |   150706 |        13405 |           231 |              4937 |       30196 |                   14421 |             16238 |              39680 |                 -19142 |      1.18625 |\n\n\n\n\nEDA (katie here)\n\n\nAnalysis\nThese functions will help us to center our data, remove data that contains errors. They will also help us plot variables on scatter plots and calculate correlation coefficients between the variables.\n\nremove_outliers(df, col_name, threshold, lower = True)\n\nOverview: This function will return a new dataframe with rows removed based on the parameters given.\nArgs:\ndf:Pandas DataFrame. This must contain columns with the names passed in as other arguments. col_name:str. Name of column contianing the incorrect data. (‚Äúcol3‚Äù) threshold:numeric. This is a numeric value to filter out values exceeding the threshold. We opted for this instead of equal to a threshold as simple functions already exist for that. (-50) lower:Bool. Indicates if filtering out all values less than, or greater than the threshold value. (True)\nImpact: Returns a pandas DataFrame. Does not write to any files.\nExample:\nOriginal df\n\nimport pandas as pd\nfrom farm_precip_project import remove_outliers\n\ndf = pd.DataFrame({\n    \"col1\": [1, 2, 3, 4],\n    \"col2\": [1, -3, 1, 1]\n})\n\nprint(df)\n\n   col1  col2\n0     1     1\n1     2    -3\n2     3     1\n3     4     1\n\n\nUsing the function\n\nimport pandas as pd\nfrom farm_precip_project import remove_outliers\n\ndf = remove_outliers(df, \"col2\", -1)\n\nprint(df)\n\n   col1  col2\n0     1     1\n2     3     1\n3     4     1\n\n\n\ncenter_column(df, col_name, col_group, col_stand_name)\n\nOverview: This function will add another column to a dataframe that centers the data by group. In other words, it will take the mean of groups within the dataset. Then, it will take the values of that column, and for each row subtract the group mean that that row pertains to.\nArgs: df:Pandas DataFrame. This must contain columns with the names passed in as other arguments. col_name:str. Name of column that will have values centered ‚Äúcol4‚Äù col_group:list. Contains strings where each string is a column to be grouped on ([‚Äúcol1‚Äù, ‚Äúcol2‚Äù, ‚Äúcol3‚Äù]) col_stand_name:str. Name for new column contains centered values. (‚ÄúcolCentered‚Äù)\nImpact:\nReturns df with an addition column for the centered data. Does not write any files.\nExample:\nOriginal df\n\nimport pandas as pd\nfrom farm_precip_project import center_column\n\ndf = pd.DataFrame({\n    \"col1\": [1, 2, 3, 4, 5, 6],\n    \"col2\": [\"A\",\"A\",\"A\",\"A\",\"A\",\"A\"]\n})\n\nprint(df)\n\n   col1 col2\n0     1    A\n1     2    A\n2     3    A\n3     4    A\n4     5    A\n5     6    A\n\n\nUsing the function\n\n# import pandas as pd\n# from farm_precip_project import remove_outliers\n\n# col_name = \"col1\"\n# col_group = [\"col2\"],\n# col_stand_name = \"col1_centered\"\n\n# df = center_column(df, col_name, col_group, col_stand_name)\n\n# print(df)"
  },
  {
    "objectID": "documentation.html#read_url_txt",
    "href": "documentation.html#read_url_txt",
    "title": "Overview",
    "section": "",
    "text": "Signature\nread_url_txt(url)\nOverview\nInput is a URL containing a .txt file. Internally uses requests.get to fetch the content, then calls write_txt() and txt_to_csv() to save the .txt and .csv files from the dataset.\nParameters - url : str\nA URL pointing to a remote text file (e.g., \"https://example.com/data.txt\").\nReturns\n- None\nThis function orchestrates fetching and saving; its side-effects are writing files to disk.\nSide Effects - Saves the fetched text as a local .txt file via write_txt(). - Converts the .txt to a parsed .csv via txt_to_csv().\nNotes - The function expects the remote resource to return HTTP status code 200. If status_code != 200, a message is printed, and the operation is aborted. - Because read_url_txt delegates to write_txt() and txt_to_csv(), it should pass the appropriate arguments such as a destination txt_name, and the parsing instructions (colspecs, cols, csv_name) to fully complete the pipeline. - Typical pitfalls: - Using a URL that does not point to a .txt file (or serves a different content type). - Forgetting to specify consistent colspecs/cols downstream, which are required by txt_to_csv().\nExample ```python import requests from your_package import read_url_txt, write_txt, txt_to_csv"
  },
  {
    "objectID": "technicalwriteup.html",
    "href": "technicalwriteup.html",
    "title": "Technical Write-up",
    "section": "",
    "text": "Check back soon"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Farm Precipitation & Income Analysis",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#import statsmodels.api as sm\n#from sklearn.linear_model import LinearRegression\n\nPull data\nFilter out 2014. There was an error that year in data collection.\n\npd.read_csv('combined_farm_precip.csv').head(1)\n\n\n\n\n\n\n\n\nstate\nyear\nValue of crop production\nCrop cash receipts\nCotton\nFeed crops\nFood grains\nFruits and nuts\nOil crops\nVegetables and melons\nAll other crops\nHome consumption\nInventory adjustment\nyearly_avg\n\n\n\n\n0\n1\n1949\n250813\n230275\n150706.0\n13405.0\n231.0\n4937.0\n30196.0\n14421.0\n16238\n39680\n-19142\n1.18625\n\n\n\n\n\n\n\n\n# df_90 = df[df[\"yearly_avg\"] &lt; -50]\n# df_90\n\n\ndf = pd.read_csv('combined_farm_precip.csv')\ncol_name = \"yearly_avg\"\nthreshold = -50\n\ndef remove_outliers(df, col_name, threshold, lower = True):\n    if lower:\n        df = df[df[col_name] &gt; threshold]\n        return df\n    else:\n        df = df[df[col_name] &lt; threshold]\n        return df\n    \ndf = remove_outliers(df, col_name, threshold, lower = True)\n\n#df[\"state\"] = pd.Categorical(df[\"state\"], categories=state_order, ordered=False)\n\n\n#df[df[\"Value of crop production\"].isna()]\n\n\n\n\nprint(\"Correlation between yearly_avg and income_stand:\")\nprint(df[\"yearly_avg\"].corr(df[\"Value of crop production\"]))\n\nprint(\"Correlation between year and Value of crop production:\")\nprint(df[\"year\"].corr(df[\"Value of crop production\"]))\n\nCorrelation between yearly_avg and income_stand:\n0.05257982662966216\nCorrelation between year and Value of crop production:\n0.3911324081866795\n\n\n\n# col_name = \"yearly_avg\"\n# col_group = \"state\"\n# col_stand_name = \"income_stand\"\n\n# def standardize_column(df, col_name, col_group, col_stand_name):\n#     mean_state_col = df.groupby(col_group)[col_name].transform('mean')\n#     std_state_col = df.groupby(col_group)[col_name].transform('std')\n#     df[col_stand_name] = (df[col_name] - mean_state_col) / std_state_col\n#     return df\n\n# df = standardize_column(df, col_name, col_group, col_stand_name)\n\n\ncol_name = \"yearly_avg\"\ncol_group = [\"state\"]\ncol_stand_name = \"income_centered\"\n\ndef center_column(df, col_name, col_group, col_stand_name):\n    mean_state_col = df.groupby(col_group)[col_name].transform('mean')\n    df[col_stand_name] = (df[col_name] - mean_state_col)\n    return df\n\ndf = center_column(df, col_name, col_group, col_stand_name)\n\n\ndef corr_and_plot(df, colx, coly, plot_file,n_digits):\n    correlation = round(df[colx].corr(df[coly]),n_digits)\n    \n    plt.figure(figsize=(8, 6))\n    plt.scatter(df[colx], df[coly])\n    plt.title(f'Scatter plot of {colx} vs {coly}, Correlation: {correlation}')\n    plt.xlabel(colx)\n    plt.ylabel(coly)\n    plt.show()\n    plt.savefig(plot_file)\n\n    return correlation\n\n\ncolx = \"yearly_avg\"\ncoly = \"income_centered\"\nplot_file = f\"plot_{colx}_vs_{coly}.png\"\nn_digits = 4\n\ncorrelation = corr_and_plot(df, colx, coly, plot_file,n_digits)\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\ncolx = \"yearly_avg\"\ncoly = \"income_stand\"\nplot_file = f\"plot_{colx}_vs_{coly}.png\"\nn_digits = 4\n\ncorrelation = corr_and_plot(df, colx, coly, plot_file,n_digits)\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n# plt.hist(df[\"income_stand\"], bins=30)\n# plt.show()\n\n# plt.plot(df[\"yearly_avg\"],df[\"Value of crop production\"],'o')\n# plt.show()\n\n# plt.plot(df[\"year\"],df[\"Value of crop production\"],'o')\n# plt.show()\n\n# plt.plot(df[\"yearly_avg\"],df[\"income_stand\"],'o')\n# plt.show()\n\n\ndf[\"yearly_avg\"].corr(df[\"income_stand\"])\n\nnp.float64(0.9661142129178085)\n\n\n\ndef make_scatter_w_cat(df, colx, coly, colcat, plot_file):\n    cat_order = sorted(df[colcat].unique())\n    palette = sns.color_palette(\"husl\", len(cat_order))\n\n    sns.scatterplot(\n    data=df,\n    x=colx,\n    y=coly,\n    hue=colcat,\n    hue_order=cat_order,\n    palette=palette,\n    s=30\n    )\n\n    leg = plt.legend(title=colcat, bbox_to_anchor=(1.02, 1), \n                     loc=\"upper left\", borderaxespad=0.,\n                     ncol = 2)\n    for text in leg.get_texts():\n        text.set_fontsize(8)  # smaller labels\n\n    plt.show()\n    plt.savefig(plot_file)\n\n\n\n\ncolx = \"yearly_avg\"\ncoly = \"income_stand\"\ncolcat = \"state\"\nplot_file = f\"plot_{colx}_vs_{coly}_by_{colcat}.png\"\n\nmake_scatter_w_cat(df, colx, coly, colcat, plot_file)\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n#df.drop([\"income_stand\",\"Value of crop production\"], axis=1).head()\n# df[\"year_numeric\"] = df[\"year\"].astype(int)\n# df_enc = pd.get_dummies(df[[\"state\", \"year_numeric\",\"yearly_avg\"]], columns=[\"state\"], drop_first=True, dtype=float)\n# df_enc = sm.add_constant(df_enc)\n# model = sm.OLS(df[\"income_stand\"], df_enc).fit()\n# print(model.summary())\n\n# summary_df = pd.DataFrame({\n#     \"term\": model.params.index,\n#     \"estimate\": model.params.values,\n#     \"std_error\": model.bse.values,\n#     \"t_value\": model.tvalues.values,\n#     \"p_value\": model.pvalues.values,\n#     \"ci_low\": model.conf_int()[0].values,\n#     \"ci_high\": model.conf_int()[1].values,\n# })\n\n# summary_df.to_csv(\"farm_regression_summary.csv\", index=False)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           income_stand   R-squared:                       0.973\nModel:                            OLS   Adj. R-squared:                  0.973\nMethod:                 Least Squares   F-statistic:                     2263.\nDate:                Fri, 12 Dec 2025   Prob (F-statistic):               0.00\nTime:                        10:23:31   Log-Likelihood:                 1235.4\nNo. Observations:                3120   AIC:                            -2371.\nDf Residuals:                    3070   BIC:                            -2069.\nDf Model:                          49                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst           -0.1897      0.315     -0.602      0.547      -0.807       0.428\nyear_numeric  6.268e-05      0.000      0.395      0.693      -0.000       0.000\nyearly_avg       0.5908      0.002    328.776      0.000       0.587       0.594\nstate_2          0.3775      0.029     13.097      0.000       0.321       0.434\nstate_3          0.0532      0.029      1.847      0.065      -0.003       0.110\nstate_4          0.2422      0.029      8.408      0.000       0.186       0.299\nstate_5          0.0420      0.029      1.458      0.145      -0.014       0.098\nstate_6          0.0290      0.029      1.008      0.314      -0.027       0.085\nstate_7          0.1866      0.029      6.480      0.000       0.130       0.243\nstate_8          0.1188      0.029      4.127      0.000       0.062       0.175\nstate_9          0.0675      0.029      2.342      0.019       0.011       0.124\nstate_10        -0.0918      0.029     -3.189      0.001      -0.148      -0.035\nstate_11        -0.1570      0.029     -5.452      0.000      -0.214      -0.101\nstate_12        -0.2364      0.029     -8.208      0.000      -0.293      -0.180\nstate_13        -0.2742      0.029     -9.518      0.000      -0.331      -0.218\nstate_14        -0.1824      0.029     -6.333      0.000      -0.239      -0.126\nstate_15        -0.0892      0.029     -3.096      0.002      -0.146      -0.033\nstate_16         0.1189      0.029      4.127      0.000       0.062       0.175\nstate_17        -0.2259      0.029     -7.841      0.000      -0.282      -0.169\nstate_18         0.1503      0.029      5.218      0.000       0.094       0.207\nstate_19        -0.1740      0.029     -6.042      0.000      -0.230      -0.118\nstate_20        -0.1406      0.029     -4.882      0.000      -0.197      -0.084\nstate_21        -0.3807      0.029    -13.208      0.000      -0.437      -0.324\nstate_22        -0.0465      0.029     -1.614      0.107      -0.103       0.010\nstate_23        -0.1051      0.029     -3.651      0.000      -0.162      -0.049\nstate_24        -0.0276      0.029     -0.959      0.338      -0.084       0.029\nstate_25        -0.3895      0.029    -13.514      0.000      -0.446      -0.333\nstate_26         0.2247      0.029      7.800      0.000       0.168       0.281\nstate_27        -0.2044      0.029     -7.095      0.000      -0.261      -0.148\nstate_28         0.0830      0.029      2.881      0.004       0.027       0.139\nstate_29         0.1787      0.029      6.203      0.000       0.122       0.235\nstate_30        -0.1948      0.029     -6.762      0.000      -0.251      -0.138\nstate_31         0.1001      0.029      3.475      0.001       0.044       0.157\nstate_32        -0.3558      0.029    -12.346      0.000      -0.412      -0.299\nstate_33        -0.1569      0.029     -5.448      0.000      -0.213      -0.100\nstate_34        -0.0562      0.029     -1.953      0.051      -0.113       0.000\nstate_35         0.1350      0.029      4.687      0.000       0.079       0.191\nstate_36        -0.1087      0.029     -3.774      0.000      -0.165      -0.052\nstate_37        -0.2040      0.029     -7.083      0.000      -0.261      -0.148\nstate_38         0.1141      0.029      3.963      0.000       0.058       0.171\nstate_39        -0.5717      0.029    -19.814      0.000      -0.628      -0.515\nstate_40        -0.1420      0.029     -4.930      0.000      -0.198      -0.086\nstate_41         0.1822      0.029      6.325      0.000       0.126       0.239\nstate_42         0.2353      0.029      8.169      0.000       0.179       0.292\nstate_43        -0.3162      0.029    -10.973      0.000      -0.373      -0.260\nstate_44         0.0457      0.029      1.585      0.113      -0.011       0.102\nstate_45        -0.0029      0.029     -0.102      0.919      -0.059       0.054\nstate_46        -0.1015      0.029     -3.524      0.000      -0.158      -0.045\nstate_47        -0.2197      0.029     -7.628      0.000      -0.276      -0.163\nstate_48         0.3472      0.029     12.048      0.000       0.291       0.404\n==============================================================================\nOmnibus:                      211.461   Durbin-Watson:                   1.159\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              844.534\nSkew:                          -0.211   Prob(JB):                    4.09e-184\nKurtosis:                       5.513   Cond. No.                     2.12e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.12e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n# farm_mdl = LinearRegression().fit(\n#     df_enc,\n#     df[\"income_stand\"]\n# )\n\n\n# print(\"R¬≤:\", farm_mdl.score(df_enc, df[\"income_stand\"]))\n# print(\"Coefficients:\", farm_mdl.coef_)\n# print(\"Intercept:\", farm_mdl.intercept_)\n\n\n\n# fitted = model.fittedvalues\n# resid = model.resid\n# #n = mpg.shape[0]\n\n# plt.scatter(fitted, resid)\n# plt.xlabel('Fitted Value')\n# plt.ylabel('Residual')\n# plt.axhline(y = 0, color = 'red', linestyle = '--')"
  },
  {
    "objectID": "index.html#about-this-project",
    "href": "index.html#about-this-project",
    "title": "Welcome to Farm Precipitation & Income Analysis",
    "section": "üéØ About This Project",
    "text": "üéØ About This Project\nThis data science project investigates the correlations between precipitation and farm income. The analysis includes:\n\nData collection from multiple sources (USDA and precipitation databases)\nComprehensive exploratory data analysis\nStatistical analysis and visualization\nDocumentation and reproducible code"
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "Welcome to Farm Precipitation & Income Analysis",
    "section": "üìä Project Overview",
    "text": "üìä Project Overview\n\nData Sources: USDA farm income data, precipitation measurements\nAnalysis Methods: Python, Pandas, statistical analysis\nVisualization: Matplotlib, Seaborn\nKey Question: How do precipitation patterns correlate with farm income?"
  },
  {
    "objectID": "index.html#documentation-resources",
    "href": "index.html#documentation-resources",
    "title": "Welcome to Farm Precipitation & Income Analysis",
    "section": "üìö Documentation & Resources",
    "text": "üìö Documentation & Resources\n\n\nDocumentation\nComplete technical documentation of the project, data sources, and methodology\n\n\nTutorial\nStep-by-step guide to get started with the project\n\n\nTechnical Write-up\nDetailed analysis results and findings from the data\n\n\n\nExplore the data and discover the relationship between precipitation and farm income!"
  }
]